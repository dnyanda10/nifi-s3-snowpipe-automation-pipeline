# NiFi-S3-Snowpipe Automation Pipeline

This project demonstrates an end-to-end **data pipeline using Apache NiFi, AWS S3, and Snowflake**, implementing **SCD Type 2** handling and **automated ingestion with Snowpipe**. It is designed to handle customer data ingestion, historical tracking, and real-time updates using cloud-native tools.


## ğŸ“Œ Project Overview

**Goal:** Automate the ingestion of customer data files from a local directory â†’ upload to S3 via NiFi â†’ auto-load into Snowflake using Snowpipe â†’ apply SCD Type 2 logic to track changes historically.


## ğŸ§± Architecture Diagram

![NiFi S3 Snowpipe Architecture](image/architecture.png)


## ğŸ› ï¸ Tools & Technologies

- **Apache NiFi 2.5**
- **AWS S3** (Data Lake)
- **AWS IAM** (for access control)
- **AWS SQS** (for Snowpipe notifications)
- **Snowflake** (Data warehouse)
- **Snowpipe** (Continuous data loading)
- **SQL (Merge, Stream, Task)** for SCD Type 2 handling


## ğŸ”„ Pipeline Flow

1. **Local File Upload:**
   - Customer `.csv` files are placed in a monitored folder.

2. **Apache NiFi Flow:**
   - `ListFile` â†’ `FetchFile` â†’ `PutS3Object`
   - Automatically pushes files from the local system to a designated S3 bucket.

3. **AWS S3 Bucket:**
   - Organized folder structure for raw data.
   - Triggers **SQS notifications** on new file upload.

4. **Snowpipe Integration:**
   - Uses SQS to trigger **Snowpipe**.
   - Snowpipe loads data into the `customer_raw` table in Snowflake.

5. **Snowflake Processing:**
   - `STREAM` tracks changes in the raw table.
   - A **view** (`v_customer_change_data`) holds CDC logic.
   - A **TASK** runs every 1 minute to merge into:
     - `customer` (current table)
     - `customer_history` (historical table)

## ğŸ§¾ Snowflake Objects Used

- Tables: `customer`, `customer_history`, `customer_raw`
- View: `v_customer_change_data`
- Stream: `customer_table_changes`
- Task: `tsk_scd_hist`
- Snowpipe: `snowpipe_customer`
- Integration: `s3_snowpipe_integration`


## ğŸ” IAM & Security

- **IAM User** with S3 access created.
- **Custom Policy** attached for:
  - S3 read/write
  - SQS read
- Trust relationship established for **Snowflake external stage**.
- Used `sqs_notification_config.json` for automated Snowpipe triggers.

---

## ğŸ“‚ Project Structure

```bash
Snowflake-NIFI real Project/
â”‚
â”œâ”€â”€ AWS S3/
â”‚   â”œâ”€â”€ bucket_structure.png
â”‚   â”œâ”€â”€ policy_file.json
â”‚   â””â”€â”€ sqs_notification_config.json
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ tables.sql
â”‚   â”œâ”€â”€ views.sql
â”‚   â”œâ”€â”€ streams.sql
â”‚   â”œâ”€â”€ tasks.sql
â”‚   â””â”€â”€ snowpipe.sql
â”‚
â”œâ”€â”€ NIFI_Template/
â”‚   â””â”€â”€ nifi_flow.xml
â”‚
â”œâ”€â”€ Image/
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

## âœ… Key Features

-Fully automated, real-time pipeline.
-Handles SCD Type 2 slowly changing dimension logic.
-No Lambda function used â€“ NiFi manages all file movement.
-Cloud-native and cost-efficient.
-Easily extendable to handle other data types or sources.

## ğŸ“Œ Challenges Faced

-IAM permission setup for Snowflakeâ€“S3 integration.
-Automating Snowpipe notifications via SQS.
-Handling dynamic updates and deletions in SCD 2 flow.
-Debugging NiFi upload and port issues.


